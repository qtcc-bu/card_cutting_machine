{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from a previous idea i had which was braindead do not use \n",
    "\n",
    "def remove_encoding_tokens(encoding_list):\n",
    "    for element in encoding_list:\n",
    "\n",
    "        if element in grammer_token_list:\n",
    "\n",
    "            encoding_list = encoding_list[encoding_list!=element]\n",
    "    return encoding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##can probably completely delete i have no idea what this does anyomore \n",
    "model_name = \"bert-base-uncased\"\n",
    "max_length = 1000\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "train_encodings = tokenizer(X, truncation=True, padding=True, max_length=max_length,is_split_into_words=False)\n",
    "train_dataset = CardDataset(train_encodings, y)\n",
    "nlp = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stole this from womewheer for some reason not sure why \n",
    "\n",
    "class CardDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_things = '.[]{)(}!@#$%^&*`|;:,<>?- \"/ '\n",
    "grammer_token_list = np.array(tokenizer.encode(grammer_things.split(),add_special_tokens=False,is_split_into_words=True))[None,:][0]\n",
    "print(grammer_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shit is probably completely irrelevant I'm pretty sure \n",
    "\n",
    "model=BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments( #TODO fuck with these \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    output_dir='.'\n",
    ")\n",
    "\n",
    "trainer = Trainer( model=model,args=training_args,train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
