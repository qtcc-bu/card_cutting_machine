{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866.9346192665695\n"
     ]
    }
   ],
   "source": [
    "# finds average card length just for funsies!\n",
    "train_data = np.array(np.load('training_data.npy',allow_pickle=True))\n",
    "if True:\n",
    "    total_length = 0\n",
    "    for card in train_data:\n",
    "        total_length +=  int(len(card[0].split()))\n",
    "print(total_length/len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is: 59677\n",
      "Min length is: 1\n",
      "Min card contents: https://open.spotify.com/track/4v55Nqito6jGwkGFOCZZ42?si=f63901d7eebf490f \n",
      "9871\n"
     ]
    }
   ],
   "source": [
    "# finds max and min (non-zero) card length also for funsies\n",
    "train_data = np.array(np.load('training_data.npy',allow_pickle=True))\n",
    "max_length = 0\n",
    "min_length = 10000000 # yeah should've used int max or wtv but i already know the max length\n",
    "min_dex = 0\n",
    "if True:\n",
    "    for index,card in enumerate(train_data):\n",
    "        if max_length < int(len(card[0].split())):\n",
    "            max_length = int(len(card[0].split()))\n",
    "        if min_length > int(len(card[0].split())) and int(len(card[0].split()))!=0:\n",
    "            min_length = int(len(card[0].split()))\n",
    "            min_dex = index\n",
    "print('Max length is: ' + str(max_length))\n",
    "print('Min length is: ' + str(min_length))\n",
    "print('Min card contents: ' + str(train_data[min_dex][0]))\n",
    "print(min_dex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35668\n"
     ]
    }
   ],
   "source": [
    "# total number of cards just to keep count\n",
    "print(len(np.load('training_data.npy',allow_pickle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# boilerplate stuff \n",
    "\n",
    "# imports! \n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast,BertForSequenceClassification,TFBertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ReformerTokenizer,ReformerModel\n",
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "# GPU meow \n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# loads the data \n",
    "train_data = np.array(np.load('training_data.npy',allow_pickle=True))\n",
    "X = list(train_data[:,0])\n",
    "y = list(train_data[:,1])\n",
    "model_name =\"bert-base-uncased\"\n",
    "#model_name = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "nlp = TFBertModel.from_pretrained(model_name)\n",
    "def convert_card_to_word_vectors(card_text,classification_values,use_bert_base=True):\n",
    "    # this ws just easier \n",
    "    #sample_text = X[card_index]\n",
    "    #class_values = y[card_index]\n",
    "    sample_text = card_text\n",
    "    class_values = classification_values\n",
    "    if use_bert_base:\n",
    "        model_name =\"bert-base-uncased\"\n",
    "    else:\n",
    "        model_name = 'bert-large-uncased'\n",
    "    ##tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "    \n",
    "    input_ids = np.array(tokenizer.encode(sample_text,add_special_tokens=False,is_split_into_words=True,truncation=True))[None,:]\n",
    "    input_ids_printable = input_ids[0]\n",
    "    # i wrote this next part of code while watching like three very drunk people watch \"Tall Girl 2\"\n",
    "    # so it's probably not going to be particularly robust or efficent \n",
    "    old_text = sample_text\n",
    "    new_text = tokenizer.batch_decode(input_ids_printable)\n",
    "    # this loop basically converts the old classification vector to be compatable with the tokens that BERT\n",
    "    # needs to work. i hate this stupid method \n",
    "    oldex = 0\n",
    "    newdex = 0\n",
    "   \n",
    "    token_mult_list = []\n",
    "    new_class_list = []\n",
    "    while oldex < len(old_text):\n",
    "        tokens_per_word = 0\n",
    "        while newdex<len(new_text) and (new_text[newdex].lower().lstrip('#') in old_text[oldex].lower()):\n",
    "            #print('meow')\n",
    "            tokens_per_word+=1\n",
    "            newdex+=1\n",
    "        token_mult_list.append(tokens_per_word)\n",
    "        oldex+=1\n",
    "    for count,value in enumerate(token_mult_list):\n",
    "        index = 0\n",
    "        while value>index:\n",
    "            new_class_list.append(class_values[count])\n",
    "            index+=1\n",
    "    #print(token_mult_list)\n",
    "    #print('Old Thing: ' + str(len(old_text)))\n",
    "    #print('New Thing: ' + str(len(new_text)))\n",
    "    #print('Mult Classes: ' + str(len(token_mult_list)))\n",
    "    #print('New Classes: ' + str(len(new_class_list)))\n",
    "\n",
    "\n",
    "    #train_encodings = np.array(tokenizer(new_text, truncation=True, padding=True, max_length=1000,is_split_into_words=True))\n",
    "    ##nlp = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    embedding = nlp(input_ids)\n",
    "    return_embedding = embedding[0][0]\n",
    "    #print(embedding[0][0])\n",
    "    if(len(new_class_list)==return_embedding.shape[0]):\n",
    "        return new_class_list,return_embedding\n",
    "    else:\n",
    "        return None,None\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    # thanks to Greenstick on StackOverflow for the method \n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 3, 1, 1, 4, 4, 3, 4, 4, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 4, 2, 2, 2, 1, 3, 3, 3, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 4, 4, 4, 4, 4, 1, 1, 1, 3, 3, 3, 4, 4, 3, 1, 1, 1], <tf.Tensor: shape=(132, 768), dtype=float32, numpy=\n",
      "array([[ 0.3447914 ,  0.38311923,  0.18867965, ..., -0.46555212,\n",
      "         0.5244969 ,  0.16768585],\n",
      "       [ 0.52054405,  0.00200855,  0.4149542 , ..., -0.59987897,\n",
      "         0.8867201 , -0.21326253],\n",
      "       [ 0.06089576, -0.2205686 ,  0.02235103, ..., -0.14053151,\n",
      "         0.3026108 ,  0.17655082],\n",
      "       ...,\n",
      "       [ 1.2743224 , -0.19928378,  0.43570018, ..., -0.34392828,\n",
      "        -0.11686718,  0.2048613 ],\n",
      "       [ 0.16592826,  0.11902209,  0.35186273, ..., -0.12351574,\n",
      "         0.01183689, -0.7323766 ],\n",
      "       [ 0.52903754, -0.46968013,  0.9103543 , ..., -0.27818233,\n",
      "         0.7701072 , -0.7532529 ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# example use of the conver_to_word_vectors method\n",
    "\n",
    "card_index = 30\n",
    "\n",
    "print(convert_card_to_word_vectors(X[card_index].split(),y[card_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35668\n"
     ]
    }
   ],
   "source": [
    "# just a place for whatever code I need for shit \n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4388█████████████████████████████████████████████████████████████████████████████████████████████████-| 100.0% \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# so you want a list of info vectors of 768 length (X) and list of class highlighting values (y) length 1 each of course\n",
    "# to get those for each card, divide into 512 (less? maybe ~450 safer) chunks and run above process on them, then for each\n",
    "# resulting vomit of info vectors, classify using a neural network or whatever in pytorch\n",
    "info_vectors = []\n",
    "class_values = []\n",
    "max_size = 450\n",
    "i = 0\n",
    "error_count = 0\n",
    "data_length = len(train_data)\n",
    "# IHC is a Mongolian CS:GO team which used to be called Checkmate, so this is like\n",
    "# a joke about checkpoint saving I guess? look I'm not exactly very smart ok give me a fucking break\n",
    "ihc_length = 3700 # should make like 10 files or so unless I fucked something up \n",
    "chunk_count = 1\n",
    "ihc_counter = 0\n",
    "\n",
    "for iter,data_pair in enumerate(train_data):\n",
    "    if ihc_counter==ihc_length:\n",
    "        np.save('info_vectors_' + str(chunk_count),info_vectors)\n",
    "        np.save('class_values_' + str(chunk_count),class_values)\n",
    "        ihc_counter=0\n",
    "        chunk_count+=1\n",
    "        info_vectors = []\n",
    "        class_values = []\n",
    "    else:\n",
    "        ihc_counter+=1\n",
    "    printProgressBar(iter,data_length)\n",
    "    #print(1,end=' ')\n",
    "    X_curr = data_pair[0].split()\n",
    "    y_curr = data_pair[1]\n",
    "    X_curr_split = [X_curr[i:i + max_size] for i in range(0, len(X_curr), max_size)]\n",
    "    y_curr_split = [y_curr[i:i + max_size] for i in range(0, len(y_curr), max_size)]\n",
    "    #print(len(X_curr_split))\n",
    "    #print(len(y_curr_split))\n",
    "    for count,chunk in enumerate(X_curr_split):\n",
    "        #print(2,end=' ')\n",
    "        new_class_list,new_word_vectors = convert_card_to_word_vectors(chunk,y_curr_split[count])\n",
    "        if new_class_list is not None:\n",
    "            for count_two, word_vect in enumerate(new_word_vectors):\n",
    "                info_vectors.append(word_vect)\n",
    "                class_values.append(new_class_list[count_two])\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "np.save('info_vectors_-1',info_vectors)\n",
    "np.save('class_values_-1',class_values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch imports and code and stuff \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "PATH = './card_net.pth'\n",
    "\n",
    "class CardNetSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(512, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 700)\n",
    "        self.fc5 = nn.Linear(700, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "class Card_Class_Data(Dataset):\n",
    "    def __init__(self, word_data):\n",
    "        self.word_data=word_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.card_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" img_name = os.path.join(self.img_path,self.img_data.loc[index, 'labels'],\n",
    "                                self.img_data.loc[index, 'Images'])\n",
    "        image = Image.open(img_name)\n",
    "        #image = image.convert('RGB')\n",
    "        image = image.resize((300,300))\n",
    "        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label \"\"\"\n",
    "        return card_index[index]\n",
    "train_loader = Card_Class_Data()#TODO add stuff here\n",
    "net = CardNetSmall()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), amsgrad=True) # hopefully if prof. cytskajfsaldfas's class taught me anything it's that ordinary SGD sucks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the training \n",
    "\n",
    "model = CardNetSmall()\n",
    "#net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#TODO get data in some configuration that doens't suck \n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss.backward()\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f996765a1a00dfcf7f072a65662b47841585989f7df806b952807dd29b522fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machinelearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
