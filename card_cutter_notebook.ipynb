{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866.9346192665695\n"
     ]
    }
   ],
   "source": [
    "# finds average card length just for funsies!\n",
    "train_data = np.array(np.load('training_data.npy',allow_pickle=True))\n",
    "if True:\n",
    "    total_length = 0\n",
    "    for card in train_data:\n",
    "        total_length +=  int(len(card[0].split()))\n",
    "print(total_length/len(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59677\n"
     ]
    }
   ],
   "source": [
    "# finds max card length also for funsies\n",
    "train_data = np.array(np.load('training_data.npy',allow_pickle=True))\n",
    "max_length = 0\n",
    "if True:\n",
    "    for card in train_data:\n",
    "        if max_length < int(len(card[0].split())):\n",
    "            max_length = int(len(card[0].split()))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# boilerplate stuff \n",
    "\n",
    "# imports! \n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast,BertForSequenceClassification,TFBertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ReformerTokenizer,ReformerModel\n",
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# GPU meow \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# loads the data \n",
    "train_data = np.array(np.load('training_data.npy',allow_pickle=True))\n",
    "X = list(train_data[:,0])\n",
    "y = list(train_data[:,1])\n",
    "model_name =\"bert-base-uncased\"\n",
    "#model_name = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "nlp = TFBertModel.from_pretrained(model_name)\n",
    "def convert_card_to_word_vectors(card_text,classification_values,use_bert_base=True):\n",
    "    # this ws just easier \n",
    "    #sample_text = X[card_index]\n",
    "    #class_values = y[card_index]\n",
    "    sample_text = card_text\n",
    "    class_values = classification_values\n",
    "    if use_bert_base:\n",
    "        model_name =\"bert-base-uncased\"\n",
    "    else:\n",
    "        model_name = 'bert-large-uncased'\n",
    "    ##tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "    \n",
    "    input_ids = np.array(tokenizer.encode(sample_text,add_special_tokens=False,is_split_into_words=True,truncation=True))[None,:]\n",
    "    input_ids_printable = input_ids[0]\n",
    "    # i wrote this next part of code while watching like three very drunk people watch \"Tall Girl 2\"\n",
    "    # so it's probably not going to be particularly robust or efficent \n",
    "    old_text = sample_text\n",
    "    new_text = tokenizer.batch_decode(input_ids_printable)\n",
    "    # this loop basically converts the old classification vector to be compatable with the tokens that BERT\n",
    "    # needs to work. i hate this stupid method \n",
    "    oldex = 0\n",
    "    newdex = 0\n",
    "   \n",
    "    token_mult_list = []\n",
    "    new_class_list = []\n",
    "    while oldex < len(old_text):\n",
    "        tokens_per_word = 0\n",
    "        while newdex<len(new_text) and (new_text[newdex].lower().lstrip('#') in old_text[oldex].lower()):\n",
    "            #print('meow')\n",
    "            tokens_per_word+=1\n",
    "            newdex+=1\n",
    "        token_mult_list.append(tokens_per_word)\n",
    "        oldex+=1\n",
    "    for count,value in enumerate(token_mult_list):\n",
    "        index = 0\n",
    "        while value>index:\n",
    "            new_class_list.append(class_values[count])\n",
    "            index+=1\n",
    "    #print(token_mult_list)\n",
    "    #print('Old Thing: ' + str(len(old_text)))\n",
    "    #print('New Thing: ' + str(len(new_text)))\n",
    "    #print('Mult Classes: ' + str(len(token_mult_list)))\n",
    "    #print('New Classes: ' + str(len(new_class_list)))\n",
    "\n",
    "\n",
    "    #train_encodings = np.array(tokenizer(new_text, truncation=True, padding=True, max_length=1000,is_split_into_words=True))\n",
    "    ##nlp = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    embedding = nlp(input_ids)\n",
    "    return_embedding = embedding[0][0]\n",
    "    #print(embedding[0][0])\n",
    "    if(len(new_class_list)==return_embedding.shape[0]):\n",
    "        return new_class_list,return_embedding\n",
    "    else:\n",
    "        return None,None\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    # thanks to Greenstick on StackOverflow for the method \n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 3, 1, 1, 4, 4, 3, 4, 4, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 4, 2, 2, 2, 1, 3, 3, 3, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 4, 4, 4, 4, 4, 1, 1, 1, 3, 3, 3, 4, 4, 3, 1, 1, 1], <tf.Tensor: shape=(132, 768), dtype=float32, numpy=\n",
      "array([[ 0.3447914 ,  0.38311923,  0.18867965, ..., -0.46555212,\n",
      "         0.5244969 ,  0.16768585],\n",
      "       [ 0.52054405,  0.00200855,  0.4149542 , ..., -0.59987897,\n",
      "         0.8867201 , -0.21326253],\n",
      "       [ 0.06089576, -0.2205686 ,  0.02235103, ..., -0.14053151,\n",
      "         0.3026108 ,  0.17655082],\n",
      "       ...,\n",
      "       [ 1.2743224 , -0.19928378,  0.43570018, ..., -0.34392828,\n",
      "        -0.11686718,  0.2048613 ],\n",
      "       [ 0.16592826,  0.11902209,  0.35186273, ..., -0.12351574,\n",
      "         0.01183689, -0.7323766 ],\n",
      "       [ 0.52903754, -0.46968013,  0.9103543 , ..., -0.27818233,\n",
      "         0.7701072 , -0.7532529 ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# example use of the conver_to_word_vectors method\n",
    "\n",
    "card_index = 30\n",
    "\n",
    "print(convert_card_to_word_vectors(X[card_index].split(),y[card_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35668\n"
     ]
    }
   ],
   "source": [
    "# just a place for whatever code I need for shit \n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |█---------------------------------------------------------------------------------------------------| 1.1% \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\qtcc\\Desktop\\Card Cutting AI\\card_cutting_machine\\card_cutter_notebook.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=15'>16</a>\u001b[0m \u001b[39m#print(len(X_curr_split))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=16'>17</a>\u001b[0m \u001b[39m#print(len(y_curr_split))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m count,chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(X_curr_split):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=18'>19</a>\u001b[0m     \u001b[39m#print(2,end=' ')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=19'>20</a>\u001b[0m     new_class_list,new_word_vectors \u001b[39m=\u001b[39m convert_card_to_word_vectors(chunk,y_curr_split[count])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m new_class_list \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000004?line=21'>22</a>\u001b[0m         \u001b[39mfor\u001b[39;00m count_two, word_vect \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(new_class_list):\n",
      "\u001b[1;32mc:\\Users\\qtcc\\Desktop\\Card Cutting AI\\card_cutting_machine\\card_cutter_notebook.ipynb Cell 3'\u001b[0m in \u001b[0;36mconvert_card_to_word_vectors\u001b[1;34m(card_text, classification_values, use_bert_base)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=61'>62</a>\u001b[0m         index\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=62'>63</a>\u001b[0m \u001b[39m#print(token_mult_list)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=63'>64</a>\u001b[0m \u001b[39m#print('Old Thing: ' + str(len(old_text)))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=64'>65</a>\u001b[0m \u001b[39m#print('New Thing: ' + str(len(new_text)))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=69'>70</a>\u001b[0m \u001b[39m#train_encodings = np.array(tokenizer(new_text, truncation=True, padding=True, max_length=1000,is_split_into_words=True))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=70'>71</a>\u001b[0m \u001b[39m##nlp = TFBertModel.from_pretrained(model_name)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=73'>74</a>\u001b[0m embedding \u001b[39m=\u001b[39m nlp(input_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=74'>75</a>\u001b[0m return_embedding \u001b[39m=\u001b[39m embedding[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qtcc/Desktop/Card%20Cutting%20AI/card_cutting_machine/card_cutter_notebook.ipynb#ch0000002?line=75'>76</a>\u001b[0m \u001b[39m#print(embedding[0][0])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1034'>1035</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1035'>1036</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1036'>1037</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1039'>1040</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\modeling_tf_utils.py:383\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/modeling_tf_utils.py?line=380'>381</a>\u001b[0m main_input \u001b[39m=\u001b[39m fn_args_and_kwargs\u001b[39m.\u001b[39mpop(main_input_name, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/modeling_tf_utils.py?line=381'>382</a>\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, main_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/modeling_tf_utils.py?line=382'>383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1109\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1063'>1064</a>\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1064'>1065</a>\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1065'>1066</a>\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1086'>1087</a>\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1087'>1088</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1088'>1089</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1089'>1090</a>\u001b[0m \u001b[39m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1090'>1091</a>\u001b[0m \u001b[39m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1106'>1107</a>\u001b[0m \u001b[39m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1107'>1108</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1108'>1109</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1109'>1110</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1110'>1111</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1111'>1112</a>\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1112'>1113</a>\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1113'>1114</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1114'>1115</a>\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1115'>1116</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1116'>1117</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1117'>1118</a>\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1118'>1119</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1119'>1120</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1120'>1121</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1121'>1122</a>\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1122'>1123</a>\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1123'>1124</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1124'>1125</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1034'>1035</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1035'>1036</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1036'>1037</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1039'>1040</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\modeling_tf_utils.py:383\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/modeling_tf_utils.py?line=380'>381</a>\u001b[0m main_input \u001b[39m=\u001b[39m fn_args_and_kwargs\u001b[39m.\u001b[39mpop(main_input_name, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/modeling_tf_utils.py?line=381'>382</a>\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, main_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/modeling_tf_utils.py?line=382'>383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:869\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=865'>866</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=866'>867</a>\u001b[0m     head_mask \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=868'>869</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=869'>870</a>\u001b[0m     hidden_states\u001b[39m=\u001b[39;49membedding_output,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=870'>871</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=871'>872</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=872'>873</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=873'>874</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=874'>875</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=875'>876</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=876'>877</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=877'>878</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=878'>879</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=879'>880</a>\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=880'>881</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=882'>883</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=883'>884</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(hidden_states\u001b[39m=\u001b[39msequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1034'>1035</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1035'>1036</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1036'>1037</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1039'>1040</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:560\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=555'>556</a>\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=557'>558</a>\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[i] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=559'>560</a>\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=560'>561</a>\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=561'>562</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=562'>563</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=563'>564</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=564'>565</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=565'>566</a>\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=566'>567</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=567'>568</a>\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=568'>569</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=569'>570</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=571'>572</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1034'>1035</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1035'>1036</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1036'>1037</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1039'>1040</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:516\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=512'>513</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=513'>514</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=515'>516</a>\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(hidden_states\u001b[39m=\u001b[39;49mattention_output)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=516'>517</a>\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_output(\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=517'>518</a>\u001b[0m     hidden_states\u001b[39m=\u001b[39mintermediate_output, input_tensor\u001b[39m=\u001b[39mattention_output, training\u001b[39m=\u001b[39mtraining\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=518'>519</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=519'>520</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1034'>1035</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1035'>1036</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1036'>1037</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1039'>1040</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:419\u001b[0m, in \u001b[0;36mTFBertIntermediate.call\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=417'>418</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: tf\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m tf\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=418'>419</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(inputs\u001b[39m=\u001b[39;49mhidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=419'>420</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/transformers/models/bert/modeling_tf_bert.py?line=421'>422</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1032'>1033</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1034'>1035</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1035'>1036</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1036'>1037</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/engine/base_layer.py?line=1039'>1040</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\keras\\layers\\core.py:1232\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/layers/core.py?line=1228'>1229</a>\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mraw_ops\u001b[39m.\u001b[39mMatMul(a\u001b[39m=\u001b[39minputs, b\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/layers/core.py?line=1229'>1230</a>\u001b[0m \u001b[39m# Broadcast kernel to inputs.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/layers/core.py?line=1230'>1231</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/layers/core.py?line=1231'>1232</a>\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mtensordot(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel, [[rank \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m], [\u001b[39m0\u001b[39;49m]])\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/layers/core.py?line=1232'>1233</a>\u001b[0m   \u001b[39m# Reshape the output back to the original ndim of the input.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/keras/layers/core.py?line=1233'>1234</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=203'>204</a>\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=204'>205</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=205'>206</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=207'>208</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=208'>209</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=209'>210</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:5042\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5038'>5039</a>\u001b[0m a_reshape, a_free_dims, a_free_dims_static \u001b[39m=\u001b[39m _tensordot_reshape(a, a_axes)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5039'>5040</a>\u001b[0m b_reshape, b_free_dims, b_free_dims_static \u001b[39m=\u001b[39m _tensordot_reshape(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5040'>5041</a>\u001b[0m     b, b_axes, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5041'>5042</a>\u001b[0m ab_matmul \u001b[39m=\u001b[39m matmul(a_reshape, b_reshape)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5042'>5043</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a_free_dims, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(b_free_dims, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5043'>5044</a>\u001b[0m   \u001b[39mif\u001b[39;00m (ab_matmul\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mis_fully_defined() \u001b[39mand\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=5044'>5045</a>\u001b[0m       ab_matmul\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mas_list() \u001b[39m==\u001b[39m a_free_dims \u001b[39m+\u001b[39m b_free_dims):\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=203'>204</a>\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=204'>205</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=205'>206</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=207'>208</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=208'>209</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/util/dispatch.py?line=209'>210</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3654\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3650'>3651</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mbatch_mat_mul_v3(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3651'>3652</a>\u001b[0m       a, b, adj_x\u001b[39m=\u001b[39madjoint_a, adj_y\u001b[39m=\u001b[39madjoint_b, Tout\u001b[39m=\u001b[39moutput_type, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3652'>3653</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3653'>3654</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3654'>3655</a>\u001b[0m       a, b, transpose_a\u001b[39m=\u001b[39;49mtranspose_a, transpose_b\u001b[39m=\u001b[39;49mtranspose_b, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\qtcc\\.conda\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:5690\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5687'>5688</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5688'>5689</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5689'>5690</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5690'>5691</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5691'>5692</a>\u001b[0m       transpose_b)\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5692'>5693</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   <a href='file:///c%3A/Users/qtcc/.conda/envs/machinelearning/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=5693'>5694</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# so you want a list of info vectors of 768 length (X) and list of class highlighting values (y) length 1 each of course\n",
    "# to get those for each card, divide into 512 (less? maybe ~450 safer) chunks and run above process on them, then for each\n",
    "# resulting vomit of info vectors, classify using a neural network or whatever in pytorch\n",
    "info_vectors = []\n",
    "class_values = []\n",
    "max_size = 450\n",
    "i = 0\n",
    "error_count = 0\n",
    "data_length = len(train_data)\n",
    "# IHC is a reference to the Mongolian CS:GO team which used to be called Checkmate, so this is like\n",
    "# a joke about checkpoint saving I guess? look I'm not exactly very smart ok give me a fucking break\n",
    "ihc_length = 370\n",
    "chunk_count = 1\n",
    "ihc_counter = 0\n",
    "\n",
    "for iter,data_pair in enumerate(train_data):\n",
    "    if ihc_counter==ihc_length:\n",
    "        np.save('info_vectors_' + str(chunk_count),info_vectors)\n",
    "        np.save('class_values_' + str(chunk_count),class_values)\n",
    "        ihc_counter=0\n",
    "        chunk_count+=1\n",
    "        info_vectors = []\n",
    "        class_values = []\n",
    "    else:\n",
    "        ihc_counter+=1\n",
    "    printProgressBar(iter,data_length)\n",
    "    #print(1,end=' ')\n",
    "    X_curr = data_pair[0].split()\n",
    "    y_curr = data_pair[1]\n",
    "    X_curr_split = [X_curr[i:i + max_size] for i in range(0, len(X_curr), max_size)]\n",
    "    y_curr_split = [y_curr[i:i + max_size] for i in range(0, len(y_curr), max_size)]\n",
    "    #print(len(X_curr_split))\n",
    "    #print(len(y_curr_split))\n",
    "    for count,chunk in enumerate(X_curr_split):\n",
    "        #print(2,end=' ')\n",
    "        new_class_list,new_word_vectors = convert_card_to_word_vectors(chunk,y_curr_split[count])\n",
    "        if new_class_list is not None:\n",
    "            for count_two, word_vect in enumerate(new_class_list):\n",
    "                info_vectors.append(word_vect)\n",
    "                class_values.append(new_class_list[count_two])\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "np.save('info_vectors',info_vectors)\n",
    "np.save('class_values',class_values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch imports and code and stuff \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "PATH = './card_net.pth'\n",
    "\n",
    "class CardNetSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(512, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 700)\n",
    "        self.fc5 = nn.Linear(700, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "class Card_Class_Data(Dataset):\n",
    "    def __init__(self, word_data):\n",
    "        self.word_data=word_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.card_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" img_name = os.path.join(self.img_path,self.img_data.loc[index, 'labels'],\n",
    "                                self.img_data.loc[index, 'Images'])\n",
    "        image = Image.open(img_name)\n",
    "        #image = image.convert('RGB')\n",
    "        image = image.resize((300,300))\n",
    "        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label \"\"\"\n",
    "        return card_index[index]\n",
    "train_loader = Card_Class_Data()#TODO add stuff here\n",
    "net = CardNetSmall()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), amsgrad=True) # hopefully if prof. cytskajfsaldfas's class taught me anything it's that ordinary SGD sucks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the training \n",
    "\n",
    "net = CardNetSmall()\n",
    "#net.load_state_dict(torch.load(PATH))\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f996765a1a00dfcf7f072a65662b47841585989f7df806b952807dd29b522fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machinelearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
